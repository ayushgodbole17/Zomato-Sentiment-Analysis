{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9941d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\godbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\godbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c9b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"new_reviews.csv\")\n",
    "\n",
    "\n",
    "vocab_size = 50000\n",
    "embedding_dim = 256\n",
    "max_length = 1000\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['reviews'], dataset['ratings'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e6ac5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " \"'\": 2,\n",
       " 'x83': 3,\n",
       " 'x82ã': 4,\n",
       " 'x82': 5,\n",
       " 'x83ã': 6,\n",
       " \"'rated\": 7,\n",
       " 'n': 8,\n",
       " \"0'\": 9,\n",
       " \"'place'\": 10,\n",
       " \"'good'\": 11,\n",
       " \"'food'\": 12,\n",
       " \"'4\": 13,\n",
       " \"'5\": 14,\n",
       " \"n'\": 15,\n",
       " \"'chicken'\": 16,\n",
       " \"'3\": 17,\n",
       " \"'service'\": 18,\n",
       " \"'ordered'\": 19,\n",
       " \"'great'\": 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938e246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 197, 3525, 242, 2, 2, 22, 216, 11760, 242, 381, 84, 645, 332, 216, 314, 3532, 2, 2, 98, 242, 411, 2, 2, 12519, 2, 2, 25, 2, 2, 2, 2, 18, 2, 2, 2, 2, 260, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 100, 2, 2, 10, 2, 2, 113, 1742, 263, 2, 2, 18, 299, 2, 2, 262, 12, 2, 2, 26, 11, 4108, 256, 2, 2, 221, 123, 2, 2, 2, 2, 2, 2, 2, 2, 755, 103, 207, 2, 2, 2, 2, 25, 393, 382, 2, 2, 13033, 1594, 880, 2, 2, 2, 2762, 11, 17397, 79, 25, 2, 2, 10, 3570, 1387, 4025, 138, 2, 2, 17, 21, 102, 2, 2, 13, 86, 2, 2, 13, 798, 2, 2, 13, 3649, 2, 2, 55, 27, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 2, 2, 7, 15, 20, 10, 626, 2, 2, 212, 499, 216, 505, 84, 588, 1, 8, 36, 12, 20, 6929, 126, 223, 4123, 8, 673, 353, 206, 49, 1792, 2171, 1387, 772, 364, 2, 2, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 286, 10, 2, 2, 12, 349, 1379, 24, 60, 2, 2, 676, 296, 2, 2, 2072, 208, 2, 2, 293, 81, 691, 2, 2, 1587, 59, 960, 2348, 22, 898, 470, 2, 2, 43, 570, 1331, 10, 512, 58, 10, 56, 733, 261, 2, 2, 2, 2, 499, 216, 238, 3175, 2, 2, 18, 690, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 2, 2, 7, 15, 10, 2692, 1071, 620, 497, 2101, 2, 2, 1240, 1543, 499, 216, 588, 567, 2, 2, 18, 12, 92, 1059, 82, 14794, 1255, 48, 2101, 29, 1488, 2, 2, 10, 10337, 2483, 2, 2, 2944, 2, 2, 19960, 338, 18, 9676, 2, 2, 358, 108, 11, 588, 663, 141, 2, 2, 2, 2, 150, 202, 483, 135, 141, 2, 2, 411, 74, 568, 1014, 10, 79, 2, 2, 2, 2, 3176, 238, 1758, 3286, 2899, 2, 2, 616, 2, 2, 885, 10056, 2, 2, 12, 109, 2, 2, 2, 2, 22732, 2, 2, 20, 10, 313, 2576, 169, 65, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 37, 9, 2, 2, 2, 7, 8, 4178, 2, 2, 4383, 2688, 2, 2, 2, 8, 3011, 2, 2, 134, 399, 2, 2, 19542, 11428, 74, 215, 319, 4376, 2, 2, 5365, 65, 1161, 2, 2, 16, 1516, 5110, 2705, 19, 2, 2, 85, 7262, 4436, 647, 2, 2, 736, 98, 433, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 2, 2, 7, 15, 68, 649, 133, 2, 2, 10, 250, 2, 2, 381, 84, 167, 20, 48, 2, 2, 1138, 204, 28, 16312, 469, 3280, 252, 2, 2, 12, 92, 11, 38, 2, 2, 28, 499, 216, 1346, 29, 645, 133, 167, 2, 2, 20, 48, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 303, 510, 2420, 128, 57, 242, 470, 7227, 3618, 126, 319, 10, 2, 2, 871, 2572, 1720, 3738, 342, 2, 2, 10, 354, 79, 170, 18840, 7651, 14759, 52, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 32, 10, 723, 12, 319, 137, 1474, 63, 1433, 2, 2, 12586, 18, 1423, 82, 83, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 11, 381, 84, 2, 2, 70, 666, 1038, 10, 231, 2, 2, 92, 52, 93, 719, 2, 2, 44, 10, 35, 2717, 2, 2, 18, 91, 292, 2, 2, 215, 1369, 195, 45, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 79, 10, 231, 2, 2, 12, 11, 3039, 615, 84, 9780, 753, 208, 16, 2, 2, 78, 816, 2, 2, 46, 6380, 2, 2, 3752, 1141, 2, 2, 1574, 9683, 3808, 2, 2, 1438, 15599, 36, 1574, 248, 2654, 2848, 78, 2245, 1696, 2, 2, 665, 12, 259, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 32, 10, 3976, 2029, 84, 2, 2, 11, 9998, 13943, 94, 2, 2, 237, 429, 2, 2, 2, 2020, 80, 16, 1516, 2, 2, 46, 616, 2, 2, 39, 72, 233, 3308, 18, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 2, 2, 7, 15, 22, 34, 231, 1533, 95, 3598, 756, 257, 14691, 772, 835, 9774, 8, 76, 153, 273, 25, 2, 2, 626, 23, 975, 241, 2, 2471, 43, 213, 1031, 255, 3576, 3268, 1457, 2878, 1088, 2, 2, 754, 51, 97, 11, 2, 2, 121, 586, 74, 1, 2234, 31, 1480, 8, 370, 34, 1518, 58, 364, 56, 1898, 28, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 2, 2, 7, 15, 41, 411, 2, 2, 22, 121, 171, 51, 411, 1387, 772, 44, 142, 29, 207, 2, 2, 149, 1069, 214, 1378, 2, 2, 126, 3172, 717, 217, 2, 2, 44, 49, 10, 1387, 772, 40, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 2, 2, 7, 15, 12, 2, 2, 17, 21, 102, 2, 2, 17, 21, 685, 2, 17, 21, 370, 11, 10, 35, 1467, 187, 31, 2, 2, 23, 82, 805, 886, 486, 2, 2, 19, 46, 2, 2, 16, 378, 132, 2, 2, 12, 29, 20, 109, 2, 2, 197, 2610, 461, 2, 2, 39, 282, 204, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 79, 10, 231, 2, 2, 439, 12, 53, 2, 2, 32, 25, 11, 84, 2, 2, 358, 11, 111, 18, 91, 292, 2, 2, 796, 2, 2, 35, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 11, 12, 2, 2, 292, 18, 2, 2, 84, 615, 2, 2, 188, 1184, 2, 2, 16, 13101, 3413, 2, 2, 379, 16, 11, 2, 2, 68, 854, 20439, 5824, 3906, 1307, 226, 118, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 2, 2, 7, 15, 564, 2414, 7932, 1164, 380, 342, 2, 2, 650, 2, 2, 1843, 2061, 2337, 133, 231, 12859, 2264, 1381, 35, 24, 11141, 221, 414, 9981, 34, 1197, 90, 2, 2, 2, 2, 633, 25, 23, 24, 11141, 644, 465, 33, 133, 1029, 2, 2, 178, 18, 24, 583, 220, 7715, 22477, 122, 155, 24, 1355, 39, 38, 5009, 38, 19437, 2, 2, 2, 2, 3783, 15345, 48, 11141, 2, 2, 84, 24, 288, 57, 2, 2, 57, 143, 317, 380, 150, 2061, 2, 2, 238, 1990, 10506, 2, 2, 2, 2, 28, 414, 143, 1114, 22478, 4033, 22479, 90, 2, 2, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 2, 2, 7, 15, 41, 411, 2, 2, 22, 121, 171, 51, 411, 1387, 772, 44, 142, 29, 207, 2, 2, 149, 1069, 214, 1378, 2, 2, 126, 3172, 717, 217, 2, 2, 44, 49, 10, 1387, 772, 40, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 2, 2, 7, 15, 12, 2, 2, 17, 21, 102, 2, 2, 17, 21, 685, 2, 17, 21, 370, 11, 10, 35, 1467, 187, 31, 2, 2, 23, 82, 805, 886, 486, 2, 2, 19, 46, 2, 2, 16, 378, 132, 2, 2, 12, 29, 20, 109, 2, 2, 197, 2610, 461, 2, 2, 39, 282, 204, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 79, 10, 231, 2, 2, 439, 12, 53, 2, 2, 32, 25, 11, 84, 2, 2, 358, 11, 111, 18, 91, 292, 2, 2, 796, 2, 2, 35, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 11, 12, 2, 2, 292, 18, 2, 2, 84, 615, 2, 2, 188, 1184, 2, 2, 16, 13101, 3413, 2, 2, 379, 16, 11, 2, 2, 68, 854, 20439, 5824, 3906, 1307, 226, 118, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 2, 2, 7, 15, 564, 2414, 7932, 1164, 380, 342, 2, 2, 650, 2, 2, 1843, 2061, 2337, 133, 231, 12859, 2264, 1381, 35, 24, 11141, 221, 414, 9981, 34, 1197, 90, 2, 2, 2, 2, 633, 25, 23, 24, 11141, 644, 465, 33, 133, 1029, 2, 2, 178, 18, 24, 583, 220, 7715, 22477, 122, 155, 24, 1355, 39, 38, 5009, 38, 19437, 2, 2, 2, 2, 3783, 15345, 48, 11141, 2, 2, 84, 24, 288, 57, 2, 2, 57, 143, 317, 380, 150, 2061, 2, 2, 238, 1990, 10506, 2, 2, 2, 2, 28, 414, 143, 1114, 22478, 4033, 22479, 90, 2, 2, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 2, 2, 7, 15, 41, 411, 2, 2, 22, 121, 171, 51, 411, 1387, 772, 44, 142, 29, 207, 2, 2, 149, 1069, 214, 1378, 2, 2, 126, 3172, 717, 217, 2, 2, 44, 49, 10, 1387, 772, 40, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 2, 2, 7, 15, 12, 2, 2, 17, 21, 102, 2, 2, 17, 21, 685, 2, 17, 21, 370, 11, 10, 35, 1467, 187, 31, 2, 2, 23, 82, 805, 886, 486, 2, 2, 19, 46, 2, 2, 16, 378, 132, 2, 2, 12, 29, 20, 109, 2, 2, 197, 2610, 461, 2, 2, 39, 282, 204, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 7, 8, 79, 10, 231, 2, 2, 439, 12, 53, 2, 2, 32, 25, 11, 84, 2, 2, 358, 11, 111, 18, 91, 292, 2, 2, 796, 2, 2, 35, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 11, 12, 2, 2, 292, 18, 2, 2, 84, 615, 2, 2, 188, 1184, 2, 2, 16, 13101, 3413, 2, 2, 379, 16, 11, 2, 2, 68, 854, 20439, 5824, 3906, 1307, 226, 118, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 2, 2, 7, 15, 564, 2414, 7932, 1164, 380, 342, 2, 2, 650, 2, 2, 1843, 2061, 2337, 133, 231, 12859, 2264, 1381, 35, 24, 11141, 221, 414, 9981, 34, 1197, 90, 2, 2, 2, 2, 633, 25, 23, 24, 11141, 644, 465, 33, 133, 1029, 2, 2, 178, 18, 24, 583, 220, 7715, 22477, 122, 155, 24, 1355, 39, 38, 5009, 38, 19437, 2, 2, 2, 2, 3783, 15345, 48, 11141, 2, 2, 84, 24, 288, 57, 2, 2, 57, 143, 317, 380, 150, 2061, 2, 2, 238, 1990, 10506, 2, 2, 2, 2, 28, 414, 143, 1114, 22478, 4033, 22479, 90, 2, 2, 40, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 613, 201, 2, 2, 105, 1566, 2591, 35, 302, 12658, 58, 7457, 2, 2, 115, 42, 805, 1261, 1120, 18, 2, 2, 3774, 592, 33, 6112, 2, 2, 564, 90, 1790, 485, 5064, 2, 2, 570, 89, 173, 2804, 3828, 480, 616, 2, 2, 12, 6122, 105, 972, 2, 2, 4469, 505, 5806, 10, 2, 2, 43, 131, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 11, 10, 420, 2, 2, 12, 183, 25, 474, 197, 1107, 2, 2, 568, 216, 25, 70, 18, 11, 11, 10, 231, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 22, 34, 1533, 1387, 772, 420, 2, 2, 802, 24, 241, 3765, 398, 25, 100, 2, 2, 11801, 16, 11435, 105, 358, 105, 1226, 1889, 106, 143, 718, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 95, 641, 715, 1533, 2, 2, 222, 2, 2, 5389, 499, 216, 2, 2, 431, 522, 2474, 10, 143, 1290, 319, 404, 2, 2, 395, 9135, 499, 33, 1415, 2, 2, 1240, 133, 1, 5492, 2, 2, 169, 1307, 2101, 1240, 3896, 373, 2839, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 39, 72, 2, 2, 400, 25, 28, 499, 216, 645, 2, 2, 497, 420, 978, 1084, 19, 316, 208, 2, 2, 296, 424, 374, 2, 2, 1755, 917, 108, 130, 5021, 57, 35, 359, 400, 302, 1084, 93, 56, 400, 10, 113, 2660, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 15, 2, 2, 8656, 2, 2, 8731, 2, 2, 5767, 2, 2, 8732, 2, 2, 4339, 2, 2, 5840, 2, 2, 1213, 2, 2, 8733, 2, 2, 7727, 2, 2, 6134, 2, 2, 8734, 2, 2, 4619, 2, 2, 8046, 2, 2, 6134, 2, 2, 2, 2, 8735, 2, 2, 8736, 2, 2, 8737, 2, 2, 5767, 2, 2, 8738, 2, 2, 124, 2, 2, 8739, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 613, 201, 2, 2, 105, 1566, 2591, 35, 302, 12658, 58, 7457, 2, 2, 115, 42, 805, 1261, 1120, 18, 2, 2, 3774, 592, 33, 6112, 2, 2, 564, 90, 1790, 485, 5064, 2, 2, 570, 89, 173, 2804, 3828, 480, 616, 2, 2, 12, 6122, 105, 972, 2, 2, 4469, 505, 5806, 10, 2, 2, 43, 131, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 11, 10, 420, 2, 2, 12, 183, 25, 474, 197, 1107, 2, 2, 568, 216, 25, 70, 18, 11, 11, 10, 231, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 22, 34, 1533, 1387, 772, 420, 2, 2, 802, 24, 241, 3765, 398, 25, 100, 2, 2, 11801, 16, 11435, 105, 358, 105, 1226, 1889, 106, 143, 718, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 95, 641, 715, 1533, 2, 2, 222, 2, 2, 5389, 499, 216, 2, 2, 431, 522, 2474, 10, 143, 1290, 319, 404, 2, 2, 395, 9135, 499, 33, 1415, 2, 2, 1240, 133, 1, 5492, 2, 2, 169, 1307, 2101, 1240, 3896, 373, 2839, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 39, 72, 2, 2, 400, 25, 28, 499, 216, 645, 2, 2, 497, 420, 978, 1084, 19, 316, 208, 2, 2, 296, 424, 374, 2, 2, 1755, 917, 108, 130, 5021, 57, 35, 359, 400, 302, 1084, 93, 56, 400, 10, 113, 2660, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 15, 2, 2, 8656, 2, 2, 8731, 2, 2, 5767, 2, 2, 8732, 2, 2, 4339, 2, 2, 5840, 2, 2, 1213, 2, 2, 8733, 2, 2, 7727, 2, 2, 6134, 2, 2, 8734, 2, 2, 4619, 2, 2, 8046, 2, 2, 6134, 2, 2, 2, 2, 8735, 2, 2, 8736, 2, 2, 8737, 2, 2, 5767, 2, 2, 8738, 2, 2, 124, 2, 2, 8739, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 2, 2, 7, 15, 7932, 1164, 380, 68, 638, 133, 2, 2, 172, 216, 2, 2, 121, 216, 499, 216, 84, 248, 315, 615, 387, 49, 2688, 2, 2, 2688, 220, 1, 92, 74, 25, 11, 644, 84, 2, 2, 1346, 29, 56, 1205, 2, 2, 12, 60, 69, 28, 995, 94, 2, 2, 6771, 624, 736, 45, 2864, 94, 785, 45, 2, 2, 29, 8504, 1622, 331, 418, 178, 4573, 190, 45, 511, 29, 1, 3256, 699, 29, 49, 118, 193, 56, 1, 102, 2, 2, 556, 23, 129, 741, 153, 10, 172, 440, 2, 2, 238, 2028, 614, 122, 12, 238, 11740, 614, 1255, 1639, 1897, 2, 2, 1387, 772, 2, 2, 254, 4383, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 68, 58, 209, 2055, 187, 2, 2, 101, 382, 2, 2, 19, 46, 2, 2, 16, 7991, 2, 2, 500, 412, 2, 2, 197, 126, 2, 2, 22, 8954, 3053, 1870, 2, 2, 26, 11, 2, 2, 94, 233, 2, 2, 2055, 10, 241, 172, 670, 971, 288, 110, 2, 2, 219, 499, 216, 288, 110, 2, 2, 1615, 4614, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 22, 34, 10, 1387, 772, 2, 2, 93, 56, 241, 626, 73, 2, 2, 5180, 283, 319, 7835, 3035, 499, 216, 2, 2, 754, 1022, 2445, 2, 2, 1536, 2, 2, 2, 7081, 2073, 211, 93, 5223, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 7932, 1164, 380, 2, 2, 2, 8, 931, 470, 10, 1387, 772, 2, 2, 2337, 133, 166, 2061, 2, 2, 3788, 2, 2, 10358, 105, 917, 2, 2, 198, 358, 2, 2, 12, 64, 2, 2, 35, 3619, 10, 376, 2, 2, 22, 2964, 1848, 13043, 493, 105, 269, 2, 2, 18, 731, 431, 2, 2, 1355, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 1077, 231, 1547, 8, 36, 1636, 10, 10131, 628, 2, 2, 2, 4380, 238, 711, 1636, 10404, 3468, 2, 2, 1636, 1987, 1170, 294, 679, 608, 1987, 2, 2, 33, 14800, 2, 2, 2, 2, 2, 2, 2, 12793, 2856, 59, 5464, 2, 2, 2, 8, 24362, 24652, 393, 63, 303, 2, 2, 1144, 979, 323, 180, 21934, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 36, 84, 11, 2, 2, 1196, 4272, 7905, 2, 2, 39, 18, 300, 2, 2, 12, 20, 83, 2, 2, 123, 1686, 2, 2, 92, 9387, 8, 1, 133, 10, 781, 11, 303, 9217, 381, 5933, 8, 86, 2, 2, 55, 21, 102, 2, 2, 17, 1905, 1432, 3112, 2, 2, 55, 21, 138, 2, 2, 55, 27, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 2, 2, 7, 15, 7932, 1164, 380, 68, 638, 133, 2, 2, 172, 216, 2, 2, 121, 216, 499, 216, 84, 248, 315, 615, 387, 49, 2688, 2, 2, 2688, 220, 1, 92, 74, 25, 11, 644, 84, 2, 2, 1346, 29, 56, 1205, 2, 2, 12, 60, 69, 28, 995, 94, 2, 2, 6771, 624, 736, 45, 2864, 94, 785, 45, 2, 2, 29, 8504, 1622, 331, 418, 178, 4573, 190, 45, 511, 29, 1, 3256, 699, 29, 49, 118, 193, 56, 1, 102, 2, 2, 556, 23, 129, 741, 153, 10, 172, 440, 2, 2, 238, 2028, 614, 122, 12, 238, 11740, 614, 1255, 1639, 1897, 2, 2, 1387, 772, 2, 2, 254, 4383, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 68, 58, 209, 2055, 187, 2, 2, 101, 382, 2, 2, 19, 46, 2, 2, 16, 7991, 2, 2, 500, 412, 2, 2, 197, 126, 2, 2, 22, 8954, 3053, 1870, 2, 2, 26, 11, 2, 2, 94, 233, 2, 2, 2055, 10, 241, 172, 670, 971, 288, 110, 2, 2, 219, 499, 216, 288, 110, 2, 2, 1615, 4614, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 22, 34, 10, 1387, 772, 2, 2, 93, 56, 241, 626, 73, 2, 2, 5180, 283, 319, 7835, 3035, 499, 216, 2, 2, 754, 1022, 2445, 2, 2, 1536, 2, 2, 2, 7081, 2073, 211, 93, 5223, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 7932, 1164, 380, 2, 2, 2, 8, 931, 470, 10, 1387, 772, 2, 2, 2337, 133, 166, 2061, 2, 2, 3788, 2, 2, 10358, 105, 917, 2, 2, 198, 358, 2, 2, 12, 64, 2, 2, 35, 3619, 10, 376, 2, 2, 22, 2964, 1848, 13043, 493, 105, 269, 2, 2, 18, 731, 431, 2, 2, 1355, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 1077, 231, 1547, 8, 36, 1636, 10, 10131, 628, 2, 2, 2, 4380, 238, 711, 1636, 10404, 3468, 2, 2, 1636, 1987, 1170, 294, 679, 608, 1987, 2, 2, 33, 14800, 2, 2, 2, 2, 2, 2, 2, 12793, 2856, 59, 5464, 2, 2, 2, 8, 24362, 24652, 393, 63, 303, 2, 2, 1144, 979, 323, 180, 21934, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 36, 84, 11, 2, 2, 1196, 4272, 7905, 2, 2, 39, 18, 300, 2, 2, 12, 20, 83, 2, 2, 123, 1686, 2, 2, 92, 9387, 8, 1, 133, 10, 781, 11, 303, 9217, 381, 5933, 8, 86, 2, 2, 55, 21, 102, 2, 2, 17, 1905, 1432, 3112, 2, 2, 55, 21, 138, 2, 2, 55, 27, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 2, 2, 7, 15, 7932, 1164, 380, 68, 638, 133, 2, 2, 172, 216, 2, 2, 121, 216, 499, 216, 84, 248, 315, 615, 387, 49, 2688, 2, 2, 2688, 220, 1, 92, 74, 25, 11, 644, 84, 2, 2, 1346, 29, 56, 1205, 2, 2, 12, 60, 69, 28, 995, 94, 2, 2, 6771, 624, 736, 45, 2864, 94, 785, 45, 2, 2, 29, 8504, 1622, 331, 418, 178, 4573, 190, 45, 511, 29, 1, 3256, 699, 29, 49, 118, 193, 56, 1, 102, 2, 2, 556, 23, 129, 741, 153, 10, 172, 440, 2, 2, 238, 2028, 614, 122, 12, 238, 11740, 614, 1255, 1639, 1897, 2, 2, 1387, 772, 2, 2, 254, 4383, 40, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 68, 58, 209, 2055, 187, 2, 2, 101, 382, 2, 2, 19, 46, 2, 2, 16, 7991, 2, 2, 500, 412, 2, 2, 197, 126, 2, 2, 22, 8954, 3053, 1870, 2, 2, 26, 11, 2, 2, 94, 233, 2, 2, 2055, 10, 241, 172, 670, 971, 288, 110, 2, 2, 219, 499, 216, 288, 110, 2, 2, 1615, 4614, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 13, 9, 2, 2, 2, 7, 8, 22, 34, 10, 1387, 772, 2, 2, 93, 56, 241, 626, 73, 2, 2, 5180, 283, 319, 7835, 3035, 499, 216, 2, 2, 754, 1022, 2445, 2, 2, 1536, 2, 2, 2, 7081, 2073, 211, 93, 5223, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 14, 9, 2, 2, 2, 7, 8, 7932, 1164, 380, 2, 2, 2, 8, 931, 470, 10, 1387, 772, 2, 2, 2337, 133, 166, 2061, 2, 2, 3788, 2, 2, 10358, 105, 917, 2, 2, 198, 358, 2, 2, 12, 64, 2, 2, 35, 3619, 10, 376, 2, 2, 22, 2964, 1848, 13043, 493, 105, 269, 2, 2, 18, 731, 431, 2, 2, 1355, 2, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 1077, 231, 1547, 8, 36, 1636, 10, 10131, 628, 2, 2, 2, 4380, 238, 711, 1636, 10404, 3468, 2, 2, 1636, 1987, 1170, 294, 679, 608, 1987, 2, 2, 33, 14800, 2, 2, 2, 2, 2, 2, 2, 12793, 2856, 59, 5464, 2, 2, 2, 8, 24362, 24652, 393, 63, 303, 2, 2, 1144, 979, 323, 180, 21934, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 36, 84, 11, 2, 2, 1196, 4272, 7905, 2, 2, 39, 18, 300, 2, 2, 12, 20, 83, 2, 2, 123, 1686, 2, 2, 92, 9387, 8, 1, 133, 10, 781, 11, 303, 9217, 381, 5933, 8, 86, 2, 2, 55, 21, 102, 2, 2, 17, 1905, 1432, 3112, 2, 2, 55, 21, 138, 2, 2, 55, 27, 2, 2, 2, 2, 2, 2, 2, 7, 17, 9, 2, 2, 2, 2, 2, 7, 15, 23, 212, 2208, 342, 3542, 149, 2490, 7821, 3081, 8969, 36, 12, 183, 2, 2, 2, 1746, 2, 2, 342, 1, 1, 2, 2, 1, 1, 307, 187, 272, 2, 2, 2, 1, 1767, 40, 2, 2, 2, 2, 2, 2, 7, 55, 9, 2, 2, 2, 7, 8, 556, 125, 83, 12, 53, 38, 69, 2, 2, 101, 1020, 2, 2, 3737, 2485, 499, 216, 172, 2, 2, 2, 2, 2610, 461, 4115, 2, 2, 355, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12996\n",
      "500\n",
      "1244\n",
      "500\n",
      "4083\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "print(train_sequences[10])\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27758cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     2     2     2     7    14     9     2     2     2     7     8\n",
      "   197  3525   242     2     2    22   216 11760   242   381    84   645\n",
      "   332   216   314  3532     2     2    98   242   411     2     2 12519\n",
      "     2     2    25     2     2     2     2    18     2     2     2     2\n",
      "   260     2     2     2     2     2     2     2     2     2     2     2\n",
      "     7    13     9     2     2     2     7     8   100     2     2    10\n",
      "     2     2   113  1742   263     2     2    18   299     2     2   262\n",
      "    12     2     2    26    11  4108   256     2     2   221   123     2\n",
      "     2     2     2     2     2     2     2   755   103   207     2     2\n",
      "     2     2    25   393   382     2     2 13033  1594   880     2     2\n",
      "     2  2762    11 17397    79    25     2     2    10  3570  1387  4025\n",
      "   138     2     2    17    21   102     2     2    13    86     2     2\n",
      "    13   798     2     2    13  3649     2     2    55    27     2     2\n",
      "     2     2     2     2     2     7    13     9     2     2     2     2\n",
      "     2     7    15    20    10   626     2     2   212   499   216   505\n",
      "    84   588     1     8    36    12    20  6929   126   223  4123     8\n",
      "   673   353   206    49  1792  2171  1387   772   364     2     2    10\n",
      "     2     2     2     2     2     2     2     2     2     2     7    13\n",
      "     9     2     2     2     7     8   286    10     2     2    12   349\n",
      "  1379    24    60     2     2   676   296     2     2  2072   208     2\n",
      "     2   293    81   691     2     2  1587    59   960  2348    22   898\n",
      "   470     2     2    43   570  1331    10   512    58    10    56   733\n",
      "   261     2     2     2     2   499   216   238  3175     2     2    18\n",
      "   690     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     7    55     9     2     2     2     2     2     7    15\n",
      "    10  2692  1071   620   497  2101     2     2  1240  1543   499   216\n",
      "   588   567     2     2    18    12    92  1059    82 14794  1255    48\n",
      "  2101    29  1488     2     2    10 10337  2483     2     2  2944     2\n",
      "     2 19960   338    18  9676     2     2   358   108    11   588   663\n",
      "   141     2     2     2     2   150   202   483   135   141     2     2\n",
      "   411    74   568  1014    10    79     2     2     2     2  3176   238\n",
      "  1758  3286  2899     2     2   616     2     2   885 10056     2     2\n",
      "    12   109     2     2     2     2 22732     2     2    20    10   313\n",
      "  2576   169    65     2     2     2     2     2     2     2     2     2\n",
      "     2     7    37     9     2     2     2     7     8  4178     2     2\n",
      "  4383  2688     2     2     2     8  3011     2     2   134   399     2\n",
      "     2 19542 11428    74   215   319  4376     2     2  5365    65  1161\n",
      "     2     2    16  1516  5110  2705    19     2     2    85  7262  4436\n",
      "   647     2     2   736    98   433     2     2     2     2     2     2\n",
      "     2     2     2     7    13     9     2     2     2     2     2     7\n",
      "    15    68   649   133     2     2    10   250     2     2   381    84\n",
      "   167    20    48     2     2  1138   204    28]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a110a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167\n",
      "(4167, 500)\n"
     ]
    }
   ],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf3f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         6400000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 512)        1050624   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512)              1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,158,406\n",
      "Trainable params: 9,158,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),\n",
    "    layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    layers.Dense(embedding_dim, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint('zomato_1.keras', save_best_only=True),\n",
    "             keras.callbacks.EarlyStopping(patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643a72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "   3/1172 [..............................] - ETA: 4:06:54 - loss: 1.6420 - accuracy: 0.4792"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, y_train, epochs=25, validation_data=(validation_padded, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
